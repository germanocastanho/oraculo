# Copyleft üÑØ, Germano Castanho, 2024
# Software livre licenciado sob a GPL-3.0
# Cada linha, um manifesto pela liberdade

import streamlit as st
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.chains import ConversationalRetrievalChain
from logica.modelos import carregar_modelo
from logica.indexador_docs import gerar_resumos_markdown
from logica.indexador_docs import criar_indice_vetorial


def criar_fluxo_conversacao():
    config = st.session_state["config"]
    modelo = carregar_modelo(config["modelo"], config["chave"])

    if not modelo:
        return

    resumos = gerar_resumos_markdown(
        modelo,
        config["diretorio"],
        """
        #### INSTRU√á√ÉO:
            
        Resuma o conte√∫do do corpus fornecido de forma clara, objetiva e concisa. O resumo deve capturar as ideias principais e os t√≥picos centrais do texto, sem incluir interpreta√ß√µes ou especula√ß√µes. 

        #### DIRETRIZES:

        - Extraia apenas as informa√ß√µes essenciais do texto, mantendo a fidelidade ao conte√∫do original.
        - O resumo deve ser estruturado como um √∫nico par√°grafo ou uma s√©rie de frases curtas e coesas.
        - Evite repetir informa√ß√µes ou usar linguagem redundante.
        - Sempre mantenha o tom t√©cnico e informativo, adequado para indexa√ß√£o em um sistema vetorial.

        #### CONTEXTO:

        - **Texto a ser resumido:** {context}

        #### RESUMO:
        """,
    )
    indice = criar_indice_vetorial(list(resumos.values()))

    prompt = PromptTemplate.from_template(
        """
        #### PERSONA:
 
        Voc√™ √© o **Or√°culo**, um assistente acad√™mico especializado em pesquisa e gerenciamento de informa√ß√µes extra√≠das de arquivos Markdown, armazenados em cofres do Obsidian. Seu objetivo √© auxiliar o usu√°rio a explorar, compreender e sintetizar informa√ß√µes relevantes do corpus fornecido.
       
        #### DIRETRIZES:
       
        - Responda exclusivamente com base no **contexto fornecido**, mantendo total fidelidade √†s informa√ß√µes dispon√≠veis no corpus. Evite especula√ß√µes ou informa√ß√µes n√£o fundamentadas.
        - Caso o contexto n√£o contenha dados suficientes, indique isso claramente e sugira alternativas, como o refinamento da pergunta do usu√°rio ou a revis√£o do corpus.
        - Use um **tom acad√™mico e t√©cnico**, estruturando respostas em par√°grafos coesos e claros.
        - Sempre que poss√≠vel, **cite diretamente** trechos ou ideias do corpus para embasar suas respostas, garantindo rastreabilidade e precis√£o.
        - Ofere√ßa respostas completas e objetivas, evitando redund√¢ncias e repeti√ß√µes desnecess√°rias.

        #### INTERA√á√ÉO:

        - **Contexto fornecido:** {context}
        - **Hist√≥rico da conversa:** {chat_history}
        - **Pergunta do usu√°rio:** {question}

        #### RESPOSTA:                      
        """
    )

    st.session_state["fluxo_conversacional"] = ConversationalRetrievalChain.from_llm(
        llm=modelo,
        memory=ConversationBufferMemory(
            memory_key="chat_history", return_messages=True
        ),
        retriever=indice.as_retriever(
            search_type="similarity", search_kwargs={"fetch_k": 20, "k": 7}
        ),
        verbose=True,
        combine_docs_chain_kwargs={"prompt": prompt},
        output_key="answer",
    )
